{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Icon Centers Dictionary:\n",
      "{'firewall.png_1': (518, 492), 'computer.png_1': (174, 359), 'router.png_1': (445, 348), 'firewall.png_2': (632, 287), 'computer.png_2': (69, 221), 'switch.png_1': (276, 213), 'server.png_1': (415, 55)}\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def ssim(img1, img2):\n",
    "    # Ensure both images are in float format for calculations\n",
    "    img1 = img1.astype(np.float64)\n",
    "    img2 = img2.astype(np.float64)\n",
    "    \n",
    "    # Set the constants for SSIM (these are standard values)\n",
    "    C1 = (0.01 * 255) ** 2\n",
    "    C2 = (0.03 * 255) ** 2\n",
    "    \n",
    "    # Compute means\n",
    "    mu1 = cv2.GaussianBlur(img1, (11, 11), 1.5)\n",
    "    mu2 = cv2.GaussianBlur(img2, (11, 11), 1.5)\n",
    "    \n",
    "    # Compute variances and covariance\n",
    "    sigma1_sq = cv2.GaussianBlur(img1 ** 2, (11, 11), 1.5) - mu1 ** 2\n",
    "    sigma2_sq = cv2.GaussianBlur(img2 ** 2, (11, 11), 1.5) - mu2 ** 2\n",
    "    sigma12 = cv2.GaussianBlur(img1 * img2, (11, 11), 1.5) - mu1 * mu2\n",
    "    \n",
    "    # Compute SSIM\n",
    "    ssim_map = ((2 * mu1 * mu2 + C1) * (2 * sigma12 + C2)) / ((mu1 ** 2 + mu2 ** 2 + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "    return ssim_map.mean()\n",
    "\n",
    "def calculate_hu_moments(image):\n",
    "    # Calculate Hu Moments which are invariant to scale, rotation, and reflection\n",
    "    moments = cv2.moments(image)\n",
    "    hu_moments = cv2.HuMoments(moments).flatten()\n",
    "    return hu_moments\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread('network.png')\n",
    "\n",
    "# Convert the image to HSV color space for better color segmentation\n",
    "hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Define a broader range for blue color\n",
    "lower_blue = np.array([90, 50, 50])  # Adjusted lower bound for a wider range\n",
    "upper_blue = np.array([140, 255, 255])  # Keep the upper bound for blue\n",
    "\n",
    "# Create a mask for blue areas in the image\n",
    "mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "\n",
    "# Apply morphological operations to clean up the mask\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n",
    "mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "# Find contours in the mask\n",
    "contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "template_dir = 'node_images'  # Change this to your actual path\n",
    "templates_hu_moments = {}\n",
    "for filename in os.listdir(template_dir):\n",
    "    template_path = os.path.join(template_dir, filename)\n",
    "    template_img = cv2.imread(template_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if template_img is not None:\n",
    "        # Calculate Hu Moments for each template and store\n",
    "        templates_hu_moments[filename] = calculate_hu_moments(template_img)\n",
    "        \n",
    "# Dictionary to store icon centers with unique labels\n",
    "icon_centers = {}\n",
    "label_counts = {}  # Dictionary to keep track of label counts\n",
    "\n",
    "# Loop through each contour and identify the closest matching icon based on Hu Moments\n",
    "for contour in contours:\n",
    "    # Get bounding box for each contour\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    \n",
    "    # Extract the region of interest (ROI) from the original image\n",
    "    roi = image[y:y+h, x:x+w]\n",
    "    \n",
    "    # Convert the ROI to grayscale\n",
    "    roi_gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculate Hu Moments for the ROI\n",
    "    roi_hu_moments = calculate_hu_moments(roi_gray)\n",
    "    \n",
    "    best_match_name = None\n",
    "    best_match_score = float('inf')  # Initialize with a high score\n",
    "    \n",
    "    # Compare Hu Moments of ROI with each template\n",
    "    for name, template_hu_moments in templates_hu_moments.items():\n",
    "        # Use Euclidean distance to compare Hu moments\n",
    "        score = np.linalg.norm(roi_hu_moments - template_hu_moments)\n",
    "        \n",
    "        # Update the best match if the score is lower\n",
    "        if score < best_match_score:\n",
    "            best_match_score = score\n",
    "            best_match_name = name\n",
    "    \n",
    "    # Additional check for computers and switches based on center intensity\n",
    "    if best_match_name == \"switch.png\":\n",
    "        center_x, center_y = w // 2, h // 2\n",
    "        center_intensity = roi_gray[center_y, center_x]\n",
    "        if center_intensity > 200:  # White center for switch\n",
    "            best_match_name = \"switch.png\"\n",
    "        else:\n",
    "            best_match_name = \"computer.png\"\n",
    "\n",
    "    # Generate a unique label by adding a count suffix\n",
    "    if best_match_name in label_counts:\n",
    "        label_counts[best_match_name] += 1\n",
    "    else:\n",
    "        label_counts[best_match_name] = 1\n",
    "    \n",
    "    unique_label = f\"{best_match_name}_{label_counts[best_match_name]}\"\n",
    "    \n",
    "    # Calculate the center of the bounding box\n",
    "    center_x = x + w // 2\n",
    "    center_y = y + h // 2\n",
    "    \n",
    "    # Store the center coordinates in the dictionary with the unique label\n",
    "    icon_centers[unique_label] = (center_x, center_y)\n",
    "\n",
    "    # Draw the bounding box and label on the image as before\n",
    "    side_length = max(w, h)\n",
    "    x = x + w // 2 - side_length // 2\n",
    "    y = y + h // 2 - side_length // 2\n",
    "    cv2.rectangle(image, (x, y), (x + side_length, y + side_length), (0, 255, 0), 2)\n",
    "    \n",
    "    label_text = unique_label\n",
    "    text_position = (x + side_length + 10, y + 20)\n",
    "    (text_width, text_height), baseline = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)\n",
    "    cv2.rectangle(image, (text_position[0] - 5, text_position[1] - text_height - 5), \n",
    "                  (text_position[0] + text_width + 5, text_position[1] + baseline - 5), \n",
    "                  (0, 0, 0), cv2.FILLED)\n",
    "    cv2.putText(image, label_text, text_position, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Save the output image\n",
    "output_path = 'boxed.png'\n",
    "cv2.imwrite(output_path, image)\n",
    "\n",
    "# Display the icon centers dictionary\n",
    "print(\"Icon Centers Dictionary:\")\n",
    "print(icon_centers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency List:\n",
      "firewall.png_1: ['router.png_1']\n",
      "computer.png_1: []\n",
      "router.png_1: ['switch.png_1', 'firewall.png_1']\n",
      "firewall.png_2: []\n",
      "computer.png_2: []\n",
      "switch.png_1: ['router.png_1']\n",
      "server.png_1: []\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def build_adjacency_list(image, icon_centers, threshold=30):\n",
    "    \"\"\"\n",
    "    Build an adjacency list representing connections between icons based on straight lines.\n",
    "    \n",
    "    Parameters:\n",
    "        image (np.array): The preprocessed binary image where lines are highlighted.\n",
    "        icon_centers (dict): Dictionary with icon labels as keys and center coordinates as values.\n",
    "        threshold (int): Distance threshold for connecting lines to icon centers.\n",
    "        \n",
    "    Returns:\n",
    "        adjacency_list (dict): An adjacency list representing the connections.\n",
    "    \"\"\"\n",
    "    adjacency_list = {label: [] for label in icon_centers.keys()}\n",
    "    \n",
    "    # Detect lines in the image using Hough Line Transform\n",
    "    lines = cv2.HoughLinesP(image, 1, np.pi / 180, 50, minLineLength=50, maxLineGap=10)\n",
    "    \n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            \n",
    "            # Identify icons near each endpoint of the line\n",
    "            start_icon = None\n",
    "            end_icon = None\n",
    "            \n",
    "            for label, (cx, cy) in icon_centers.items():\n",
    "                # Check if the start point of the line is close to an icon center\n",
    "                if np.sqrt((x1 - cx) ** 2 + (y1 - cy) ** 2) < threshold:\n",
    "                    start_icon = label\n",
    "                \n",
    "                # Check if the end point of the line is close to an icon center\n",
    "                if np.sqrt((x2 - cx) ** 2 + (y2 - cy) ** 2) < threshold:\n",
    "                    end_icon = label\n",
    "            \n",
    "            # If the line connects two different icons, add them to the adjacency list\n",
    "            if start_icon and end_icon and start_icon != end_icon:\n",
    "                if end_icon not in adjacency_list[start_icon]:\n",
    "                    adjacency_list[start_icon].append(end_icon)\n",
    "                if start_icon not in adjacency_list[end_icon]:\n",
    "                    adjacency_list[end_icon].append(start_icon)\n",
    "\n",
    "    return adjacency_list\n",
    "\n",
    "# Preprocess the image to highlight lines for line detection\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "edges = cv2.Canny(gray, 50, 150)  # Detect edges\n",
    "\n",
    "# Build the adjacency list based on detected connections\n",
    "adjacency_list = build_adjacency_list(edges, icon_centers)\n",
    "\n",
    "print(\"Adjacency List:\")\n",
    "for node, connections in adjacency_list.items():\n",
    "    print(f\"{node}: {connections}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
